import scrapy
from scrapy_playwright.page import PageMethod
from ..items import HAItem  # Asegúrate de que tu item HAItem está bien definido

class HousinganywhereSpider(scrapy.Spider):
    name = "housinganywhere"
    allowed_domains = ["housinganywhere.com"]

    # Genera URLs de paginación (ajusta el rango según necesites)
    start_urls = [f'https://housinganywhere.com/es/s/Barcelona--Espa%C3%B1a?page={i}' for i in range(1, 20)]
    
    custom_settings = {
        'FEEDS': {
            'data/housinganywhere.csv': {
                'format': 'csv',
                'encoding': 'utf8',
                'overwrite': True
            }
        }
    }

    def start_requests(self):
        for url in self.start_urls:
            yield scrapy.Request(
                url,
                meta={
                    "playwright": True,
                    # Aumentamos los tiempos de espera para que se cargue el contenido dinámico
                    "playwright_page_methods": [
                        PageMethod("wait_for_selector", "a.css-1efwqj7-cardLink"), 
                        PageMethod("evaluate", "window.scrollBy(0, window.innerHeight)"),
                        PageMethod("evaluate", "window.scrollBy(0, window.innerHeight)"),
                        PageMethod("wait_for_timeout", 300),
                        PageMethod("evaluate", "window.scrollBy(0, window.innerHeight)"),
                        PageMethod("wait_for_timeout", 200),
                        PageMethod("evaluate", "window.scrollBy(0, window.innerHeight)"),
                        PageMethod("wait_for_timeout", 500),
                        PageMethod("evaluate", "window.scrollBy(0, window.innerHeight)"),
                        # PageMethod("evaluate", "window.scrollTo(0, document.body.scrollHeight)"),
                        PageMethod("wait_for_timeout", 3000),
                    ]
                },
                callback=self.parse
            )

    def parse(self, response):
        # Aquí obtenemos Selectors, no strings:
        links = response.css("a.css-1efwqj7-cardLink")

        self.log(f"Encontrados {len(links)} enlaces en {response.url}")

        for link in links:
            item = HAItem()
            item["url_actual"] = response.url

            href = link.attrib.get("href")
            if not href:
                continue

            item["url_inmueble"] = response.urljoin(href).split("?")[0]

            # Título usando regex, pero ahora RELATIVO al <a>
            item["title"] = link.xpath(
                ".//span[re:test(@data-test-locator, 'Listing.*Card.*Title', 'i')]/text()"
            ).get() #La i en tu expresión XPath corresponde a "case-insensitive" (insensible a mayúsculas/minúsculas).

            # Precio: puedes hacerlo con contains o también con regex, como prefieras
            item["price"] = link.xpath(
                ".//span[contains(@data-test-locator, 'Price')]/text()"
            ).get()

            yield item



